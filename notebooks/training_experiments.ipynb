{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/processed_data.pt\"\n",
    "data = torch.load(data_path)\n",
    "\n",
    "prompts, image_paths = data[\"prompts\"], data[\"image_paths\"]\n",
    "\n",
    "print(f\"Количество записей: {len(prompts)}\")\n",
    "print(f\"Пример запроса: {prompts[0]}\")\n",
    "print(f\"Пример пути к изображению: {image_paths[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просмотр первых нескольких изображений\n",
    "for i in range(5):\n",
    "    img = Image.open(image_paths[i])\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(prompts[i])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\"path_to_pretrained_unet\")\n",
    "\n",
    "scheduler = DDPMScheduler.from_pretrained(\"path_to_pretrained_scheduler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "unet = unet.to(device)\n",
    "text_encoder = text_encoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 5\n",
    "\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n",
    "transform = Compose([\n",
    "    Resize((256, 256)),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = [\n",
    "    (prompts[i], transform(torch.tensor(np.array(Image.open(image_paths[i])))))\n",
    "    for i in range(len(prompts))\n",
    "]\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for prompts_batch, images_batch in tqdm(dataloader, desc=f\"Epoch {epoch}\"):\n",
    "        inputs = tokenizer(prompts_batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        images_batch = images_batch.to(device)\n",
    "\n",
    "        noise = torch.randn_like(images_batch).to(device)\n",
    "        noisy_images = scheduler.add_noise(images_batch, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        noise_pred = unet(noisy_images, text_embeds=text_encoder(**inputs).last_hidden_state).sample\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {epoch_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [0.123, 0.110, 0.105, 0.098, 0.090]  # Пример данных\n",
    "\n",
    "plt.plot(range(1, len(losses) + 1), losses, marker=\"o\")\n",
    "plt.title(\"График потерь (Loss)\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация тестового изображения\n",
    "test_prompt = \"A tribal tattoo design with sharp edges\"\n",
    "inputs = tokenizer([test_prompt], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "# Генерация\n",
    "noise = torch.randn((1, 3, 256, 256)).to(device)\n",
    "generated_image = unet(noise, text_embeds=text_encoder(**inputs).last_hidden_state).sample\n",
    "\n",
    "# Преобразование изображения для отображения\n",
    "generated_image = (generated_image[0].cpu().detach().numpy() * 0.5 + 0.5).transpose(1, 2, 0)\n",
    "plt.imshow(generated_image)\n",
    "plt.title(test_prompt)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
